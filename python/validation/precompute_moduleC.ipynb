{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "a7de104f",
            "metadata": {},
            "source": [
                "# Module C — Precompute Validation\n",
                "This notebook mirrors the `make precompute-data` target. It orchestrates the canonical 12 configuration runs, captures metadata, and ensures the generated artifacts under `python/validation/artifacts/precompute_data/` stay in sync with the Rust core implementation."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "15d66107",
            "metadata": {},
            "source": [
                "## 1. Configure Validation Paths and Environment\n",
                "Resolve paths relative to this notebook, hydrate optional environment variables, and surface the validation binary along with the artifact directory used throughout this run."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f1b6cd94",
            "metadata": {},
            "outputs": [],
            "source": [
                "from __future__ import annotations\n",
                "import json\n",
                "import os\n",
                "import subprocess\n",
                "import time\n",
                "from dataclasses import dataclass\n",
                "from pathlib import Path\n",
                "from typing import Any\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import plotly.express as px\n",
                "import plotly.graph_objects as go\n",
                "from dotenv import load_dotenv\n",
                "from IPython.display import HTML, display\n",
                "from plotly.subplots import make_subplots\n",
                "\n",
                "# Load optional .env to pick up VALIDATION_BIN overrides, etc.\n",
                "ENV_PATH = Path.cwd() / \".env\"\n",
                "if ENV_PATH.exists():\n",
                "    load_dotenv(ENV_PATH)\n",
                "\n",
                "try:\n",
                "    NOTEBOOK_DIR = Path(__file__).resolve().parent\n",
                "except NameError:\n",
                "    # __file__ is not defined when running inside some notebook frontends,\n",
                "    # so fall back to the working directory to keep relative paths stable.\n",
                "    NOTEBOOK_DIR = Path.cwd()\n",
                "\n",
                "REPO_ROOT = NOTEBOOK_DIR.parent.parent\n",
                "VALIDATION_DIR = NOTEBOOK_DIR\n",
                "ARTIFACT_DIR = VALIDATION_DIR / \"artifacts\" / \"precompute_data\"\n",
                "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "VALIDATION_BIN = os.environ.get(\"VALIDATION_BIN\", \"mpb2d-validation\")\n",
                "CARGO_CMD = os.environ.get(\"CARGO\", \"cargo\")\n",
                "print(f\"Notebook dir: {NOTEBOOK_DIR}\")\n",
                "print(f\"Artifact dir: {ARTIFACT_DIR}\")\n",
                "print(f\"Validation binary: {VALIDATION_BIN}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "09e68fee",
            "metadata": {},
            "source": [
                "## 2. Declare Precompute Parameter Grid\n",
                "Mirror the Makefile tuples (lattice, resolution, radii, permittivities, mesh sizes) so the notebook can orchestrate the same runs deterministically."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0d2d956c",
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_param_grid() -> pd.DataFrame:\n",
                "    entries = [\n",
                "        # Square lattice tuples\n",
                "        dict(lattice=\"square\", resolution=24, radius=0.28, eps_bg=11.0, eps_inside=1.0, mesh_size=1, tag=\"square_res24_mesh1\"),\n",
                "        dict(lattice=\"square\", resolution=24, radius=0.28, eps_bg=11.0, eps_inside=1.0, mesh_size=4, tag=\"square_res24_mesh4\"),\n",
                "        dict(lattice=\"square\", resolution=48, radius=0.25, eps_bg=8.5, eps_inside=2.0, mesh_size=1, tag=\"square_res48_mesh1\"),\n",
                "        dict(lattice=\"square\", resolution=48, radius=0.25, eps_bg=8.5, eps_inside=2.0, mesh_size=4, tag=\"square_res48_mesh4\"),\n",
                "        dict(lattice=\"square\", resolution=128, radius=0.20, eps_bg=16.0, eps_inside=4.0, mesh_size=1, tag=\"square_res128_mesh1\"),\n",
                "        dict(lattice=\"square\", resolution=128, radius=0.20, eps_bg=16.0, eps_inside=4.0, mesh_size=4, tag=\"square_res128_mesh4\"),\n",
                "        # Triangular lattice tuples\n",
                "        dict(lattice=\"triangular\", resolution=24, radius=0.22, eps_bg=10.5, eps_inside=3.5, mesh_size=1, tag=\"triangular_res24_mesh1\"),\n",
                "        dict(lattice=\"triangular\", resolution=24, radius=0.22, eps_bg=10.5, eps_inside=3.5, mesh_size=4, tag=\"triangular_res24_mesh4\"),\n",
                "        dict(lattice=\"triangular\", resolution=48, radius=0.18, eps_bg=6.0, eps_inside=1.5, mesh_size=1, tag=\"triangular_res48_mesh1\"),\n",
                "        dict(lattice=\"triangular\", resolution=48, radius=0.18, eps_bg=6.0, eps_inside=1.5, mesh_size=4, tag=\"triangular_res48_mesh4\"),\n",
                "        dict(lattice=\"triangular\", resolution=128, radius=0.15, eps_bg=14.0, eps_inside=5.5, mesh_size=1, tag=\"triangular_res128_mesh1\"),\n",
                "        dict(lattice=\"triangular\", resolution=128, radius=0.15, eps_bg=14.0, eps_inside=5.5, mesh_size=4, tag=\"triangular_res128_mesh4\"),\n",
                "    ]\n",
                "    df_params = pd.DataFrame(entries)\n",
                "    df_params[\"output_json\"] = df_params.apply(\n",
                "        lambda row: f\"precompute_{row.lattice}_res{row.resolution}_bg{row.eps_bg:.1f}_hole{row.eps_inside:.1f}_mesh{row.mesh_size}.json\",\n",
                "        axis=1,\n",
                "    )\n",
                "    df_params[\"output_path\"] = df_params[\"output_json\"].apply(lambda name: ARTIFACT_DIR / name)\n",
                "    return df_params\n",
                "\n",
                "params_df = build_param_grid()\n",
                "params_df"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8a38cf1a",
            "metadata": {},
            "source": [
                "## 3. Helper to Invoke Cargo Validation Binary\n",
                "Wrap `cargo run -p mpb2d-validation -- precompute-data …` with structured logging so every run surfaces its arguments, runtime, and stdout/stderr for later inspection."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "699d15b9",
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class RunResult:\n",
                "    tag: str\n",
                "    lattice: str\n",
                "    resolution: int\n",
                "    mesh_size: int\n",
                "    output_path: Path\n",
                "    duration_s: float\n",
                "    returncode: int\n",
                "    stdout_path: Path\n",
                "    stderr_path: Path\n",
                "\n",
                "def run_precompute_row(row: pd.Series) -> RunResult:\n",
                "    args = [\n",
                "        CARGO_CMD,\n",
                "        \"run\",\n",
                "        \"-p\", VALIDATION_BIN,\n",
                "        \"--\", \"precompute-data\",\n",
                "        \"--lattice\", row.lattice,\n",
                "        \"--resolution\", str(row.resolution),\n",
                "        \"--radius\", str(row.radius),\n",
                "        \"--eps-bg\", str(row.eps_bg),\n",
                "        \"--eps-inside\", str(row.eps_inside),\n",
                "        \"--mesh-size\", str(row.mesh_size),\n",
                "        \"--output\", str(row.output_path),\n",
                "        \"--tag\", row.tag,\n",
                "    ]\n",
                "    stdout_path = ARTIFACT_DIR / f\"{row.tag}_stdout.log\"\n",
                "    stderr_path = ARTIFACT_DIR / f\"{row.tag}_stderr.log\"\n",
                "    start = time.perf_counter()\n",
                "    completed = subprocess.run(\n",
                "        args,\n",
                "        cwd=REPO_ROOT,\n",
                "        capture_output=True,\n",
                "        text=True,\n",
                "        check=False,\n",
                "    )\n",
                "    duration = time.perf_counter() - start\n",
                "    stdout_path.write_text(completed.stdout)\n",
                "    stderr_path.write_text(completed.stderr)\n",
                "    return RunResult(\n",
                "        tag=row.tag,\n",
                "        lattice=row.lattice,\n",
                "        resolution=row.resolution,\n",
                "        mesh_size=row.mesh_size,\n",
                "        output_path=row.output_path,\n",
                "        duration_s=duration,\n",
                "        returncode=completed.returncode,\n",
                "        stdout_path=stdout_path,\n",
                "        stderr_path=stderr_path,\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9694ea3b",
            "metadata": {},
            "source": [
                "## 4. Batch Execution Across Square and Triangular Lattices\n",
                "Iterate over the parameter grid, run each configuration exactly once, and capture timing plus success metrics. The logs are persisted next to the artifacts for later debugging."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a17de717",
            "metadata": {},
            "outputs": [],
            "source": [
                "results: list[RunResult] = []\n",
                "for _, row in params_df.iterrows():\n",
                "    print(f\"Running {row.tag} …\", end=\" \")\n",
                "    run_result = run_precompute_row(row)\n",
                "    status = \"OK\" if run_result.returncode == 0 else f\"FAIL ({run_result.returncode})\"\n",
                "    print(status)\n",
                "    results.append(run_result)\n",
                "\n",
                "results_df = pd.DataFrame([r.__dict__ for r in results])\n",
                "results_df"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6ef76857",
            "metadata": {},
            "source": [
                "## 5. Validate Generated JSON Artifacts\n",
                "Confirm every expected JSON file exists, then spot-check a subset to ensure the schema exposes TE/TM effective epsilons, FFT stats, and file references."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "71dc520b",
            "metadata": {},
            "outputs": [],
            "source": [
                "def validate_json_artifacts(params: pd.DataFrame, sample_size: int = 3) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
                "    missing_json: list[Path] = []\n",
                "    missing_csv: list[Path] = []\n",
                "    summaries: list[dict[str, Any]] = []\n",
                "    samples: list[dict[str, Any]] = []\n",
                "    for _, row in params.iterrows():\n",
                "        json_path: Path = row.output_path\n",
                "        if not json_path.exists():\n",
                "            missing_json.append(json_path)\n",
                "            continue\n",
                "        payload = json.loads(json_path.read_text())\n",
                "        files = payload.get(\"files\", {})\n",
                "        eps_csv = files.get(\"epsilon_fourier_csv\")\n",
                "        fft_csv = files.get(\"fft_workspace_csv\")\n",
                "        eps_ok = bool(eps_csv and json_path.with_name(eps_csv).exists())\n",
                "        fft_ok = bool(fft_csv and json_path.with_name(fft_csv).exists())\n",
                "        if eps_csv and not eps_ok:\n",
                "            missing_csv.append(json_path.with_name(eps_csv))\n",
                "        if fft_csv and not fft_ok:\n",
                "            missing_csv.append(json_path.with_name(fft_csv))\n",
                "        summaries.append(\n",
                "        {\n",
                "            \"tag\": payload.get(\"tag\") or row.tag,\n",
                "            \"lattice\": payload.get(\"lattice\"),\n",
                "            \"resolution\": payload.get(\"resolution\"),\n",
                "            \"mesh_size\": payload.get(\"mesh_size\"),\n",
                "            \"te_eps_eff\": payload.get(\"te_eps_eff\"),\n",
                "            \"tm_eps_eff\": payload.get(\"tm_eps_eff\"),\n",
                "            \"clamp_fraction\": payload.get(\"clamp_fraction\"),\n",
                "            \"has_eps_fourier_csv\": eps_ok,\n",
                "            \"has_fft_workspace_csv\": fft_ok,\n",
                "            \"json_path\": str(json_path),\n",
                "            \"output_path\": str(json_path),\n",
                "        },\n",
                "        )\n",
                "        if len(samples) < sample_size:\n",
                "            samples.append(\n",
                "            {\n",
                "                \"tag\": payload.get(\"tag\") or row.tag,\n",
                "                \"te_eps_eff\": payload.get(\"te_eps_eff\"),\n",
                "                \"tm_eps_eff\": payload.get(\"tm_eps_eff\"),\n",
                "                \"k_plus_g_sq\": payload.get(\"k_plus_g_sq\"),\n",
                "                \"workspace\": payload.get(\"workspace\"),\n",
                "                \"files\": files,\n",
                "            },\n",
                "            )\n",
                "    if missing_json:\n",
                "        raise FileNotFoundError(f\"Missing JSON artifacts: {[str(p) for p in missing_json]}\")\n",
                "    if missing_csv:\n",
                "        raise FileNotFoundError(f\"Referenced CSV artifacts not found: {[str(p) for p in missing_csv]}\")\n",
                "    return pd.DataFrame(summaries), pd.DataFrame(samples)\n",
                "\n",
                "artifact_status_df, sample_payloads_df = validate_json_artifacts(params_df)\n",
                "artifact_status_df, sample_payloads_df"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9499506c",
            "metadata": {},
            "source": [
                "## 6. Persist Run + Artifact Metadata\n",
                "Merge the runtime metadata with artifact stats and persist compact CSV/JSON summaries so downstream notebooks can ingest the same information without re-running the expensive simulations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d34e516b",
            "metadata": {},
            "outputs": [],
            "source": [
                "def _prepare_run_df() -> pd.DataFrame:\n",
                "    if \"results_df\" in globals():\n",
                "        df = results_df.copy()\n",
                "    else:\n",
                "        df = params_df[[\"tag\", \"lattice\", \"resolution\", \"mesh_size\", \"output_path\"]].copy()\n",
                "        df[\"duration_s\"] = pd.NA\n",
                "        df[\"returncode\"] = pd.NA\n",
                "        df[\"stdout_path\"] = pd.NA\n",
                "        df[\"stderr_path\"] = pd.NA\n",
                "    for col in [\"output_path\", \"stdout_path\", \"stderr_path\"]:\n",
                "        if col in df.columns:\n",
                "            df[col] = df[col].apply(lambda value: str(value) if isinstance(value, Path) else value)\n",
                "    return df\n",
                "\n",
                "def persist_metadata(run_df: pd.DataFrame, artifacts_df: pd.DataFrame) -> dict[str, Path]:\n",
                "    artifacts_df = artifacts_df.copy()\n",
                "    if \"output_path\" in artifacts_df.columns and \"json_path\" in artifacts_df.columns:\n",
                "        artifacts_df.drop(columns=[\"output_path\"], inplace=True)\n",
                "    if \"json_path\" in artifacts_df.columns:\n",
                "        artifacts_df.rename(columns={\"json_path\": \"output_path\"}, inplace=True)\n",
                "    if \"output_path\" not in artifacts_df.columns:\n",
                "        raise ValueError(\"artifact_status_df must contain an output_path or json_path column\")\n",
                "    artifacts_df[\"output_path\"] = artifacts_df[\"output_path\"].astype(str)\n",
                "    run_df = run_df.copy()\n",
                "    merged = artifacts_df.merge(\n",
                "        run_df,\n",
                "        on=[\"tag\", \"lattice\", \"resolution\", \"mesh_size\", \"output_path\"],\n",
                "        how=\"left\",\n",
                "        suffixes=(\"\", \"_run\"),\n",
                "    )\n",
                "    artifact_status_path = ARTIFACT_DIR / \"precompute_artifact_status.csv\"\n",
                "    merged_path_csv = ARTIFACT_DIR / \"precompute_run_metadata.csv\"\n",
                "    merged_path_json = ARTIFACT_DIR / \"precompute_run_metadata.json\"\n",
                "    artifacts_df.to_csv(artifact_status_path, index=False)\n",
                "    merged.to_csv(merged_path_csv, index=False)\n",
                "    merged.to_json(merged_path_json, orient=\"records\", indent=2)\n",
                "    return {\n",
                "        \"artifact_status_csv\": artifact_status_path,\n",
                "        \"run_metadata_csv\": merged_path_csv,\n",
                "        \"run_metadata_json\": merged_path_json,\n",
                "    }\n",
                "\n",
                "run_df_prepared = _prepare_run_df()\n",
                "output_paths = persist_metadata(run_df_prepared, artifact_status_df)\n",
                "output_paths"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9311deae",
            "metadata": {},
            "source": [
                "## 7. Visualize ε(G) Spectra and FFT Workspace Grids\n",
                "Build per-configuration heatmaps for the complex ε(G) spectrum (magnitude) and the FFT workspace diagnostics (|k+G|² plus clamp mask). Title each figure with the TE/TM effective permittivities to quickly compare materials across the parameter grid."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d80c1bed",
            "metadata": {},
            "outputs": [],
            "source": [
                "def _pivot_matrix(df: pd.DataFrame, value_col: str) -> np.ndarray:\n",
                "    pivot = df.pivot(index=\"iy\", columns=\"ix\", values=value_col).sort_index(ascending=False)\n",
                "    return pivot.values\n",
                "\n",
                "def _safe_show(fig):\n",
                "    try:\n",
                "        fig.show()\n",
                "    except ValueError as exc:\n",
                "        if \"nbformat\" in str(exc).lower():\n",
                "            display(HTML(fig.to_html(include_plotlyjs=\"cdn\", full_html=False)))\n",
                "        else:\n",
                "            raise\n",
                "\n",
                "def _load_epsilon_fourier_maps(csv_path: Path) -> dict[str, np.ndarray]:\n",
                "    df = pd.read_csv(csv_path)\n",
                "    for col in (\"real\", \"imag\"):\n",
                "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
                "    df[\"magnitude\"] = np.sqrt(df[\"real\"].fillna(0.0) ** 2 + df[\"imag\"].fillna(0.0) ** 2)\n",
                "    return {\n",
                "        \"magnitude\": _pivot_matrix(df, \"magnitude\"),\n",
                "        \"real\": _pivot_matrix(df, \"real\"),\n",
                "        \"imag\": _pivot_matrix(df, \"imag\"),\n",
                "    }\n",
                "\n",
                "def _load_fft_workspace_maps(csv_path: Path) -> dict[str, np.ndarray]:\n",
                "    df = pd.read_csv(csv_path)\n",
                "    numeric_cols = [\"k_plus_g_sq\", \"kx_plus_g\", \"ky_plus_g\"]\n",
                "    for col in numeric_cols:\n",
                "        if col in df.columns:\n",
                "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
                "    df[\"clamped\"] = df[\"clamped\"].astype(int)\n",
                "    return {\n",
                "        \"k_plus_g_sq\": _pivot_matrix(df, \"k_plus_g_sq\"),\n",
                "        \"clamped\": _pivot_matrix(df, \"clamped\"),\n",
                "    }\n",
                "\n",
                "def _resolve_artifacts(row: pd.Series) -> tuple[dict[str, Any], Path, Path]:\n",
                "    json_path = Path(row[\"output_path\"])\n",
                "    payload = json.loads(json_path.read_text())\n",
                "    files = payload.get(\"files\", {})\n",
                "    eps_csv = files.get(\"epsilon_fourier_csv\")\n",
                "    fft_csv = files.get(\"fft_workspace_csv\")\n",
                "    if not eps_csv or not fft_csv:\n",
                "        raise FileNotFoundError(f\"CSV references missing for {row['tag']}\")\n",
                "    eps_path = json_path.with_name(eps_csv)\n",
                "    fft_path = json_path.with_name(fft_csv)\n",
                "    if not eps_path.exists():\n",
                "        raise FileNotFoundError(f\"Missing epsilon(G) CSV: {eps_path}\")\n",
                "    if not fft_path.exists():\n",
                "        raise FileNotFoundError(f\"Missing FFT workspace CSV: {fft_path}\")\n",
                "    return payload, eps_path, fft_path\n",
                "\n",
                "def plot_config_artifacts(row: pd.Series) -> None:\n",
                "    payload, eps_path, fft_path = _resolve_artifacts(row)\n",
                "    eps_maps = _load_epsilon_fourier_maps(eps_path)\n",
                "    fft_maps = _load_fft_workspace_maps(fft_path)\n",
                "    title_suffix = f\"TE eff={payload['te_eps_eff']:.3f} · TM eff={payload['tm_eps_eff']:.3f}\"\n",
                "    fig_eps = make_subplots(\n",
                "        rows=1, cols=3,\n",
                "        subplot_titles=(\"|ε(G)|\", \"Re ε(G)\", \"Im ε(G)\"),\n",
                "        horizontal_spacing=0.05,\n",
                "    )\n",
                "    fig_eps.add_trace(go.Heatmap(z=eps_maps[\"magnitude\"], coloraxis=\"coloraxis\"), row=1, col=1)\n",
                "    fig_eps.add_trace(go.Heatmap(z=eps_maps[\"real\"], coloraxis=\"coloraxis2\"), row=1, col=2)\n",
                "    fig_eps.add_trace(go.Heatmap(z=eps_maps[\"imag\"], coloraxis=\"coloraxis3\"), row=1, col=3)\n",
                "    fig_eps.update_layout(\n",
                "        title=f\"{row['tag']} — ε(G) spectra ({title_suffix})\",\n",
                "        height=450,\n",
                "        width=1350,\n",
                "        coloraxis=dict(colorscale=\"Viridis\", colorbar_title=\"|ε(G)|\"),\n",
                "        coloraxis2=dict(\n",
                "            colorscale=\"RdBu\",\n",
                "            colorbar=dict(title=\"Re ε(G)\", x=0.53, xanchor=\"left\", len=0.8, xpad=30)\n",
                "        ),\n",
                "        coloraxis3=dict(colorscale=\"PuOr\", colorbar_title=\"Im\", colorbar=dict(x=0.93)),\n",
                "    )\n",
                "    _safe_show(fig_eps)\n",
                "\n",
                "    fig_fft = make_subplots(\n",
                "        rows=1, cols=2,\n",
                "        subplot_titles=(\"k+G squared\", \"Clamp mask\"),\n",
                "        horizontal_spacing=0.08,\n",
                "    )\n",
                "    fig_fft.add_trace(go.Heatmap(z=fft_maps[\"k_plus_g_sq\"], colorscale=\"Magma\", colorbar=dict(title=\"|k+G|²\")), row=1, col=1)\n",
                "    fig_fft.add_trace(\n",
                "        go.Heatmap(\n",
                "            z=fft_maps[\"clamped\"],\n",
                "            colorscale=[[0.0, \"#1f77b4\"], [1.0, \"#d62728\"]],\n",
                "            showscale=False,\n",
                "        ),\n",
                "        row=1,\n",
                "        col=2,\n",
                "    )\n",
                "    fig_fft.update_layout(\n",
                "        title=f\"{row['tag']} — FFT workspace diagnostics ({title_suffix})\",\n",
                "        height=400,\n",
                "        width=900,\n",
                "    )\n",
                "    _safe_show(fig_fft)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c9dc8443",
            "metadata": {},
            "outputs": [],
            "source": [
                "def render_all_artifact_plots(order_by: tuple[str, ...] = (\"lattice\", \"resolution\", \"mesh_size\"), limit: int | None = None) -> None:\n",
                "    if \"artifact_status_df\" not in globals():\n",
                "        raise RuntimeError(\"Run section 5 first to populate artifact_status_df\")\n",
                "    ordered = artifact_status_df.sort_values(list(order_by)).reset_index(drop=True)\n",
                "    rows = ordered.head(limit) if limit else ordered\n",
                "    for idx, row in rows.iterrows():\n",
                "        print(f\"Rendering plots for {row['tag']} ({idx + 1}/{len(rows)})\")\n",
                "        try:\n",
                "            plot_config_artifacts(row)\n",
                "        except FileNotFoundError as exc:\n",
                "            print(f\"  Skipping due to missing files: {exc}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fb756c1d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Render every configuration (set limit for quicker spot-checks)\n",
                "render_all_artifact_plots()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "mpb-reference",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
