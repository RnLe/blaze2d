# 2D MPB Pipeline

1. **High-level pipeline**: “from parameters in → band diagram out”, in modular blocks.
2. **Detailed physics & algorithms**: master equation, TE/TM splitting, ε(r) + smoothing, FFT operators, preconditioner, deflation, iterative eigensolver.

No code, only structure and minimal pseudocode where it helps.

<!-- markdownlint-disable MD036 MD029 MD023 MD007 MD024 -->

---

## 1. High-level 2D MPB-lite pipeline

### 1.1 Inputs

Conceptually, the solver takes:

* **Lattice data**

  * Real-space lattice vectors (a_1, a_2 \in \mathbb{R}^2)
  * (Derived) reciprocal vectors (b_1, b_2)
  * Lattice type tag: square, hex, rectangular, oblique (just affects convenience functions & k-paths)

#### Checklist — Lattice data

* [x] Existence & optionality — confirm `Lattice2D` exposes defaults for common lattices, flags when reciprocal vectors are missing/miscomputed, and allows toggling custom vs preset paths. (files: `crates/core/src/lattice.rs`, `crates/validation/src/main.rs`)
* [x] Plotting & logging — emit lattice summary tables and optional SVG/Matplotlib plots of real vs reciprocal vectors for sanity checks. (files: `python/validation/lattice_validation.ipynb`, `crates/validation/src/main.rs`)
* [x] Data validation — numerically verify a_i · b_j = 2π δ_ij and that k-path endpoints stay inside the first Brillouin zone. (files: `crates/validation/src/main.rs`)
* [x] Testing implemented — unit tests covering square/hex/oblique lattice construction, reciprocal derivation, and invalid input rejection. (files: `crates/core/src/_tests_lattice.rs`)

* **Geometry & materials**

  * Background dielectric (\varepsilon_\text{bg})
  * List of inclusions (“atoms”)

    * Fractional positions ((u_i,v_i)) in unit cell
    * Shape type (for now: circle with radius (r_i))
    * Material (\varepsilon_i) (air holes → ε=1)
  * (Optional) multiple materials, overlapping shapes

#### Checklist — Geometry & materials

* [x] Existence & optionality — ensure geometry parser can be disabled for analytic test cases, supports optional overlapping primitives, and surfaces defaults for absent material parameters. (files: `crates/core/src/geometry.rs`, `crates/core/src/_tests_geometry.rs`)
* [x] Plotting & logging — capture voxelized ε(r) snapshots, per-material volume fractions, and overlap warnings in logs/plots. (files: `crates/validation/src/validation_geometry.rs`, `python/validation/geometry_validation.ipynb`, `python/validation/README.md`)
* [x] Data validation — compare computed fill factors and centroid positions against analytic expectations for canonical cells (e.g., circular holes at Γ). (files: `crates/validation/src/validation_geometry.rs`, `python/validation/artifacts/geometry_data/`)
* [x] Testing implemented — regression fixtures that reconstruct ε(r) for sample TOML inputs and check hashes/statistics against golden data. (files: `crates/core/src/_tests_geometry.rs`)
* Provenance — Notebook -> Makefile -> `mpb2d-validation` -> `Geometry2D`: Cell 2 of `python/validation/geometry_validation.ipynb` loads `geometry_*.json` artifacts generated by `make geometry-data`, which calls `cargo run -p mpb2d-validation -- geometry-data …` (see `python/validation/Makefile`). That subcommand (`crates/validation/src/main.rs`) executes `validation_geometry::run()` and samples ε via `Geometry2D::relative_permittivity_at_fractional` from `mpb2d_core::geometry`. The production pipeline (`mpb2d-cli`) feeds the same `Geometry2D` into `Dielectric2D::from_geometry()` (`crates/core/src/dielectric.rs`) before dumping `epsilon_real.csv`, so notebook plots reflect the same source data (subject to any smoothing mesh size you enable at runtime).

* **Numerics & smoothing**

  * Grid size (N_x \times N_y)
  * Smoothing parameters (e.g. `mesh_size` analogue)
  * Polarization: TE or TM
  * Number of bands to compute
  * Tolerances, max iterations

#### Checklist — Numerics & smoothing

* [x] Existence & optionality — CLI flags (`crates/cli/src/main.rs`) feed into `JobConfig` (`crates/core/src/io.rs`), which stores `Grid2D`, `Polarization`, band counts, tolerances, max iterations, and `DielectricOptions::smoothing`. `bandstructure::run_with_metrics()` ( `crates/core/src/bandstructure.rs` ) consumes those values verbatim.
* [x] Plotting & logging — `bandstructure.rs` logs grid dimensions, polarization, smoothing mesh_size, and tolerance at startup; enabling `--dump-pipeline` persists `epsilon_real*.csv` snapshots that capture the actual ε field used in operator assembly. `python/validation/geometry_validation.ipynb` (Cells 5–6) visualizes the smoothing sweep, complementing the CLI logs.
* [x] Data validation — `python/validation/Makefile` now exposes `make smoothing-data`, which drives `mpb2d-validation -- smoothing-data …` across the six canonical tuples plus the triangular `N=48` mesh_size sweep (1/4/12). The notebook ensures each artifact’s grid shape matches `N×N`, compares smoothed vs raw averages, and plots Δε, confirming mesh_size propagation from Makefile → CLI → JSON → visualization.
* [x] Testing implemented — `crates/core/src/_tests_dielectric.rs` and `crates/backend-cpu/src/_tests_lib.rs` exercise `Dielectric2D::from_geometry()` with and without smoothing, verifying averages, tensor construction, and polarization-dependent consumers. These tests run across multiple mesh_size values to guard against regression in setup logic.
* Provenance — Grid size/band-count/polarization knobs originate in TOML configs or CLI overrides, flow through `JobConfig` serialization, and are echoed by `bandstructure.rs` before sampling ε. When `--dump-pipeline` is on, the emitted `epsilon_real.csv` shares the same dimensions/stats as the validation artifacts (`python/validation/README.md`, `geometry_validation.ipynb`), so the numbers observed at the notebook level are guaranteed to match the ones driving the eigensolver.

* **k-space sampling**

  * Either a k-mesh (for 2D band surfaces)
  * Or a k-path (Γ–X–M–Γ, Γ–M–K–Γ, etc.)

#### Checklist — k-space sampling

* [x] Existence & optionality — `mpb2d-validation -- k-space-data …` (see `crates/validation/src/validation_kspace.rs`) now accepts `--mode mesh|path`, lattice-aware defaults (Γ–X–M–Γ for square, Γ–M–K–Γ for triangular), and `--custom-path` strings to disable presets. The `python/validation/Makefile` exposes `make kspace-data` to persist canonical artifacts.
* [x] Plotting & logging — `python/validation/kspace_validation.ipynb` overlays every generated mesh/path on a 3×3 reciprocal lattice grid, annotates segment names, and lists cumulative path lengths, satisfying the visualization+logging requirements.
* [x] Data validation — each JSON report includes segment-level alignment checks, uniqueness counts, Gamma wrap-back assertions, and mesh coverage summaries (see `python/validation/artifacts/kspace_data/*.json`).
* [x] Testing implemented — unit tests inside `validation_kspace.rs` guard mesh uniqueness, custom-path parsing, and preset segment alignment.

---

### 1.2 Pipeline modules (conceptual blocks)

**Module A — Lattice & k-path**

* Build `Lattice2D` with (a_1, a_2)
* Compute reciprocal lattice `ReciprocalLattice2D` with (b_1, b_2)
* Construct list of k-points (k_j) (path or mesh) in reduced coordinates

#### Checklist — Module A (Lattice & k-path)

* [x] Existence & optionality — the new `k-space-data` validation subcommand ( `crates/validation/src/validation_kspace.rs` ) produces either meshes or symmetry paths on demand, with lattice-aware defaults and full overrides for custom coordinate lists.
* [x] Plotting & logging — `python/validation/kspace_validation.ipynb` renders reciprocal grids plus the requested paths/meshes, labeling segments and sampling density for inspection; artifacts capture the underlying coordinates and cumulative distances for logging.
* [x] Data validation — JSON reports under `python/validation/artifacts/kspace_data/` encode adjacency checks, Γ wrap-back validation, and reciprocal basis metadata, ensuring differences between successive k_j samples match expectations.
* [x] Testing implemented — the `validation_kspace.rs` unit tests cover mesh uniqueness, custom-path parsing, and the Γ–X–M–Γ densification logic, providing regression coverage for Module A.

**Module B — Geometry → ε(r)**

* Build a uniform **real-space grid** for one unit cell:

  * Points (r_{ij}) for (i=0..N_x-1, j=0..N_y-1)
* For each grid point, evaluate **scalar ε(r)** from geometry:

  * Determine which material(s) the point belongs to (background vs hole)
  * For interface voxels, compute **sub-pixel averages** to get a smoothed ε(r) (possibly anisotropic, but in 2D TE/TM we effectively store scalar ε and ε⁻¹ or equivalent)

Output: arrays ε(r) and, if needed, ε⁻¹(r) on the grid.

#### Checklist — Module B (Geometry → ε(r))

* [x] Existence & optionality — ensure geometry evaluation can be bypassed when loading cached ε fields, and allow optional storage of ε⁻¹ depending on polarization needs. (files: `crates/core/src/geometry.rs`, `crates/core/src/_tests_geometry.rs`)
* [x] Plotting & logging — dump min/max ε statistics, histograms, and representative grid heatmaps per material. (files: `crates/validation/src/validation_geometry.rs`, `python/validation/geometry_validation.ipynb`)
* [x] Data validation — cross-validate voxel membership via analytical area fractions and confirm ε(r) periodicity by comparing opposing cell faces. (files: `crates/validation/src/validation_geometry.rs`, `python/validation/artifacts/geometry_data/`)
* [x] Testing implemented — golden master tests that serialize ε(r) for sample cells and compare hashes across architectures. (files: `crates/core/src/_tests_geometry.rs`)

**Module C — Precomputation (geometry-independent of k)**

* Precompute FFT plans for size (N_x \times N_y)
* Precompute any static geometry-dependent maps (e.g. ε_eff used in preconditioner)

#### Checklist — Module C (Precomputation)

* [x] Existence & optionality — `mpb2d-validation -- precompute-data` plus `ThetaOperator::build_*preconditioner` in `crates/core/src/operator.rs` share the same FFT planning + ε_eff caching knobs, and both paths can skip precompute outputs (no `--output`) for diagnostics or persist JSON/CSV artifacts for production.
* [x] Plotting & logging — the CLI report records FFT backend, `k+G` stats, clamp fractions, and artifact filenames; `python/validation/precompute_moduleC.ipynb` and its `precompute_visualization.ipynb` companion render ε(G) / |k+G|² heatmaps with TE/TM effective ε badges, covering the “log & plot” requirement.
* [x] Data validation — the notebook validates every JSON reference, checks CSV presence, and replays the TE/TM effective ε plus clamp metrics, while the visualization notebook spot-checks all 12 canonical configs to ensure smoothed spectra behave as expected.
* [x] Testing implemented — Fourier-diagonal preconditioner tests in `crates/core/src/_tests_operator.rs` exercise the ε_eff plumbing, and the validation CLI is wired into `python/validation/Makefile` targets (`make precompute-data`) for repeatable regression coverage.

**Module D — For each polarization (TE, TM) and each k:**

For each k-point:

1. **Reciprocal indexing & k+G**

   * For each Fourier index (ix,iy) map to integer wave numbers (m_1, m_2)
   * Compute (G = m_1 b_1 + m_2 b_2)
   * Compute (q = k + G)
   * Store (|q|^2) and related quantities

#### Checklist — Reciprocal indexing & k+G

* [ ] Existence & optionality — ensure index remapping handles both centered and standard FFT layouts and can be reused across polarizations when geometry is shared.
* [ ] Plotting & logging — dump histograms of |q| values and spot-check q-vectors for boundary modes.
* [ ] Data validation — verify q tables satisfy periodic aliasing constraints and that |q|^2 never goes negative due to precision issues.
* [ ] Testing implemented — deterministic tests comparing generated q-grids against stored references for small N_x,N_y cases.

2. **Operator assembly (A,B)**

   * Define how to apply (A_k) to a field u (this is a *matvec*, not a dense matrix)
   * Define the B-inner product and B-action (B u)
   * For TE and TM, these differ slightly

#### Checklist — Operator assembly (A,B)

* [ ] Existence & optionality — confirm matvec closures are registered for both TE and TM, and allow disabling TM assembly when only TE bands are requested.
* [ ] Plotting & logging — log operator norm estimates, sparsity/FFT call counts, and whether real/complex shortcuts were applied.
* [ ] Data validation — compare A u against analytical derivatives for plane-wave test fields and ensure B remains Hermitian positive definite numerically.
* [ ] Testing implemented — property tests that feed random vectors through A and B and check adjoint consistency via inner products.

3. **Preconditioner M_k**

   * Build diagonal (in Fourier basis) preconditioner (M^{-1}) approximating (A_k^{-1})

#### Checklist — Preconditioner M_k

* [ ] Existence & optionality — allow switching between homogeneous Fourier preconditioner, identity, or user-supplied variants via config.
* [ ] Plotting & logging — record spectral bounds of diagonal entries and ratios between preconditioned and raw residual norms.
* [ ] Data validation — ensure zero-mode handling matches deflation settings and that denominators stay above stability thresholds.
* [ ] Testing implemented — tests that compare convergence rates with and without preconditioning on small benchmark problems.

4. **Deflation & symmetry constraints**

   * Build deflation subspace (e.g. constant mode at Γ; possibly previous bands)
   * Build parity projectors if using mirror symmetries

#### Checklist — Deflation & symmetry constraints

* [ ] Existence & optionality — verify constant-mode deflation auto-activates at Γ while symmetry projectors remain opt-in per lattice.
* [ ] Plotting & logging — log deflated subspace dimensions, symmetry tags, and projection residual norms.
* [ ] Data validation — check B-orthogonality of deflation bases and confirm symmetry projections are idempotent.
* [ ] Testing implemented — tests that inject known symmetric fields and assert the projections recover expected parity components.

5. **Iterative eigensolver**

   * Run block LOBPCG / CG in the B-inner product
   * Each iteration uses:

     * y = A_k x
     * r = y − λ B x
     * z = M^{-1} r
     * projections (deflation + parity)
   * Converge lowest N_bands eigenpairs

  #### Checklist — Iterative eigensolver

  * [ ] Existence & optionality — confirm block size, tolerance, and max-iteration knobs are exposed and default to stable values, with ability to switch solvers (e.g., LOBPCG vs CG).
  * [ ] Plotting & logging — emit per-iteration residual norms, Rayleigh quotient traces, and stagnation diagnostics suitable for plotting.
  * [ ] Data validation — verify residuals shrink monotonically for well-conditioned runs and detect divergence to trigger failsafes.
  * [ ] Testing implemented — CI test that runs mini band problems (both TE/TM) and asserts convergence within iteration/time budgets.

6. **Store results**

   * For each k, store eigenvalues (\omega_n(k)) (or λ_n = (ω/c)²)
   * Optionally store eigenvectors for analysis

#### Checklist — Store results

* [ ] Existence & optionality — ensure eigenvalue storage is mandatory while eigenvector dumps are gated behind flags to save memory.
* [ ] Plotting & logging — record band gaps, min/max frequencies, and optionally serialize spectra for downstream plotting scripts.
* [ ] Data validation — cross-check stored ω_n(k) against symmetry-related k-points and enforce sorting consistency across the path.
* [ ] Testing implemented — snapshot tests comparing serialized band structures against trusted MPB outputs for baseline cells.

**Module E — Postprocessing**

* Combine {k_j, ω_n(k_j)} into:

  * Band diagram along k-path
  * Band surfaces on k-mesh
* Export to CSV, HDF5, JSON, or send to visualization frontend

#### Checklist — Module E (Postprocessing)

* [ ] Existence & optionality — support opt-in exporters (CSV/JSON/HDF5) with configurable precision and allow disabling heavy outputs.
* [ ] Plotting & logging — produce quick-look band plots, group velocity tables, and log file locations for generated artifacts.
* [ ] Data validation — verify continuity of bands along the k-path and re-run simple gap-detection algorithms to flag anomalies.
* [ ] Testing implemented — tests that load exported files and compare schema/content against reference expectations.

---

### 1.3 What goes in / out

* **Input**: Problem definition = (lattice, geometry, ε/μ, polarization, grid, k-sampling, solver options)
* **Intermediate**: ε(r), FFT plans, k+G arrays, operator application & preconditioner functions
* **Output**:

  * Band frequencies {ω_n(k)} (and optionally eigenfields)
  * Derived quantities (band gaps, group velocities, etc., later)

---

## 2. Detailed physics & algorithms

Now the “utter detail” version.

### 2.1 Master equation & Bloch theorem

Start from full Maxwell in frequency domain (no sources, μ=1):

[
\nabla \times \left(\varepsilon^{-1}(\mathbf r),\nabla \times \mathbf H(\mathbf r)\right)
= \left(\frac{\omega^2}{c^2}\right)\mathbf H(\mathbf r).
]

Assume **2D periodicity** in x-y, invariance in z, periodic ε(x,y), constant in z; fields can be separated into polarized cases:

* **TE-like**: H has only Hz component (out of plane) or equivalently E is in plane;
* **TM-like**: E has Ez component; there are different naming conventions, but we define explicitly:

Let’s adopt:

* **TM case**: out-of-plane (H_z(x,y)) is nonzero; leads to scalar equation with ε inside the operator.
* **TE case**: out-of-plane (E_z(x,y)) is nonzero; leads to scalar equation with ε as a weight.

Using Bloch’s theorem for periodic media:

[
u(\mathbf r) = e^{i\mathbf k\cdot \mathbf r} \tilde{u}(\mathbf r),
]
with (\tilde{u}(\mathbf r + \mathbf R) = \tilde{u}(\mathbf r)) periodic in the lattice.

In discretization, it is convenient to work directly with the **periodic part** (\tilde{u}(\mathbf r)) on one cell and treat k as a shift in derivatives.

---

#### Checklist — Master equation & Bloch theorem

* [ ] Existence & optionality — confirm μ=1 and z-invariant assumptions are configurable and document fallback when 3D terms are disabled.
* [ ] Plotting & logging — log selected polarization, material assumptions, and any gauge choices when instantiating the master equation.
* [ ] Data validation — numerically verify Bloch periodicity by checking u(r+R) = e^{ik·R} u(r) for representative R vectors during debug runs.
* [ ] Testing implemented — symbolic/regression tests deriving TE/TM scalar forms from the full Maxwell operator to catch algebraic regressions.

### 2.2 TE/TM scalar equations in 2D

For **2D, z-invariant, μ=1**, standard derivations give:

* **TE (E_z polarization)**:

  * E = (0,0,E_z), H in plane.

  * Governing equation:

    [
    \nabla\cdot(\nabla E_z) + \omega^2 \varepsilon(\mathbf r) E_z = 0
    \quad \Rightarrow \quad
    -\nabla^2 E_z = \left(\frac{\omega^2}{c^2}\right)\varepsilon(\mathbf r) E_z.
    ]

  * Generalized eigenproblem:

    [
    A E_z = \lambda B E_z,\quad
    A = -\nabla^2,\quad B = \varepsilon(\mathbf r),\quad \lambda = \omega^2/c^2.
    ]

* **TM (H_z polarization)**:

  * H = (0,0,H_z), E in plane.

  * Governing equation:

    [
    \nabla\cdot\left(\varepsilon^{-1}(\mathbf r)\nabla H_z\right) + \omega^2 H_z = 0
    \quad\Rightarrow\quad
    -\nabla\cdot\left(\varepsilon^{-1}(\mathbf r)\nabla H_z\right)
    = \left(\frac{\omega^2}{c^2}\right)H_z.
    ]

  * Generalized eigenproblem:

    [
    A H_z = \lambda B H_z,\quad
    A = -\nabla\cdot\varepsilon^{-1}(\mathbf r)\nabla,\quad B = I,\quad \lambda=\omega^2/c^2.
    ]

We want to solve these for each Bloch k.

#### Bloch shift

In periodic setting, derivatives become:

[
\nabla \rightarrow \nabla + i\mathbf k,
]

so for TE:

[
-\left(\nabla + i\mathbf k\right)^2 E_z = \lambda \varepsilon E_z,
]

for TM:

[
-\left(\nabla + i\mathbf k\right)\cdot\left(\varepsilon^{-1} (\nabla + i\mathbf k)\right) H_z = \lambda H_z.
]

In Fourier representation, (\nabla \leftrightarrow i\mathbf G); Bloch shift just adds k:

[
\nabla + i\mathbf k \leftrightarrow i(\mathbf k + \mathbf G).
]

---

#### Checklist — TE/TM scalar equations

* [ ] Existence & optionality — ensure both TE and TM scalar branches can be toggled independently and default polarization matches CLI/API requests.
* [ ] Plotting & logging — record which generalized eigenproblem (A,B) pair is instantiated along with conditioning estimates.
* [ ] Data validation — substitute analytic plane-wave solutions to confirm each branch reproduces expected eigenvalues for uniform ε.
* [ ] Testing implemented — regression tests that compare TE/TM spectra against MPB benchmarks for simple lattices.

### 2.3 Geometry → ε(r) and smoothing

For one unit cell:

1. **Define real-space grid**

   * Choose (N_x, N_y); grid points:

     [
     \mathbf r_{ij} = i,\frac{a_1}{N_x} + j,\frac{a_2}{N_y},\quad i=0..N_x-1,\ j=0..N_y-1.
     ]

2. **Evaluate “raw” ε(r)**

   For each grid point:

   * Map r to fractional coordinates (u,v) s.t. (r = u a_1 + v a_2), with u,v in [0,1).
   * For each inclusion (circle centered at (u_i,v_i), radius r_i):

     * Compute Cartesian center (r_i = u_i a_1 + v_i a_2).
     * Compute distance (|r−r_i|).
     * Flag if inside hole; assign appropriate ε (air vs background).
   * With multiple inclusions/materials, union / priority rules decide ε(r).

   This gives a **piecewise constant** ε_raw(r), with discontinuous jumps.

3. **Sub-pixel smoothing**

   MPB’s approach: to reduce Gibbs artifacts and improve convergence of plane-wave methods, each grid voxel gets an **effective permittivity** computed by sampling geometry at sub-pixel scale and applying effective medium theory.

   For each voxel:

   * Subdivide voxel into microcells (controlled by `mesh_size`).

   * Compute:

     * volume average (\langle\varepsilon\rangle),
     * harmonic average (\langle\varepsilon^{-1}\rangle),
     * “dipole” vector of dielectric contrast → interface normal (\mathbf n).

   * Build projector (P_{ij} = n_i n_j) and (I-P).

   * Construct effective permittivity tensor:

     [
     \tilde{\varepsilon}^{-1}
     = P,\langle\varepsilon^{-1}\rangle + (I-P),\langle\varepsilon\rangle^{-1}.
     ]

   * In 2D scalar TE/TM, this reduces to using different effective ε or ε⁻¹ for derivatives along / across interface, but numerically you can still treat it as scalar if you only store an effective average consistent with your polarization.

   Smoothing can be **optional** (mesh_size=1 → no smoothing).

Result: arrays ε(r) (and ε⁻¹(r)) on the grid, potentially with sub-pixel effective values near boundaries.

---

#### Checklist — Geometry → ε(r) & smoothing

* [ ] Existence & optionality — allow smoothing to be disabled (`mesh_size=1`), swap smoothing kernels, or load cached ε fields.
* [ ] Plotting & logging — capture before/after smoothing slices, interface normals, and summary stats (min/max/avg ε, volume fractions).
* [ ] Data validation — ensure sub-pixel averaging preserves total energy (∑ε vs ∑ε⁻¹) and that normals remain unit-length.
* [ ] Testing implemented — tests that vary mesh_size and verify convergence of effective ε against analytic values for simple geometries.

### 2.4 Discretization and FFT machinery

We represent fields and ε on a **uniform grid** and use FFTs for derivatives.

#### Real vs Fourier representation

* Real-space field u(x,y) → discrete samples (u_{ij}).
* Fourier-space field (\hat u_{mn}) corresponds to reciprocal lattice vectors:

  [
  \mathbf G_{mn} = m b_1 + n b_2,
  ]
  where integer indices m,n map to FFT index pairs.

DFT conventions are chosen so that:

* forward FFT: (u_{ij} \to \hat u_{mn}),
* inverse FFT: (\hat u_{mn} \to u_{ij}).

You need a consistent mapping between FFT indices (0..N_x-1) and Fourier integers m = −N_x/2..N_x/2−1, etc.

#### Bloch shift

Derivatives are implemented via (i(\mathbf k+\mathbf G)):

* For each Fourier mode (m,n), compute:

  [
  \mathbf q_{mn} = \mathbf k + \mathbf G_{mn},
  \quad |q_{mn}|^2 = q_{x}^2 + q_{y}^2.
  ]

Store these arrays once per k.

---

#### Checklist — Discretization & FFT machinery

* [ ] Existence & optionality — support multiple FFT backends (CPU/GPU) and allow falling back to finite differences for debugging.
* [ ] Plotting & logging — log DFT normalization conventions, padding strategies, and any aliasing warnings.
* [ ] Data validation — perform forward-then-inverse FFT round trips on random fields to confirm unitary scaling and index mapping.
* [ ] Testing implemented — tests that compare FFT-based derivative stencils against finite-difference references for small grids.

### 2.5 Operator application A u (TE and TM)

Everything in the eigensolver is built on **matvec** operations: given u, compute y = A u.

#### TE operator

From:

[
A E_z = -(\nabla+i\mathbf k)^2 E_z, \quad B E_z = \varepsilon E_z.
]

Pseudo-spectral implementation:

1. Start with E in real space: E(r).

2. FFT → E_G.

3. For each G:

   [
   \widehat{A E}_G = |q_G|^2 E_G, \quad q_G = k+G.
   ]

4. Inverse FFT → AE(r) if we want real space representation, or keep in Fourier for some parts.

The mass matrix B acts in real space as multiplication by ε(r). In B-inner products:

[
(x,y)*B = \sum*{ij} x_{ij}^*,\varepsilon_{ij},y_{ij},\Delta V.
]

#### TM operator

From:

[
A H_z = -(\nabla+i\mathbf k)\cdot\varepsilon^{-1}(\mathbf r)(\nabla + i\mathbf k) H_z,\quad B H_z = H_z.
]

Implementation:

1. Start with H(r).

2. FFT → H_G.

3. Compute gradient in Fourier space:

   [
   \widehat{\partial_x H} = i q_x(G), H_G,\quad
   \widehat{\partial_y H} = i q_y(G), H_G.
   ]

4. IFFT → (\partial_x H(r), \partial_y H(r)).

5. Multiply by ε⁻¹(r) in real space:

   [
   f_x(r) = \varepsilon^{-1}(r),\partial_x H(r),\quad
   f_y(r) = \varepsilon^{-1}(r),\partial_y H(r).
   ]

6. FFT f_x, f_y → (\widehat{f_x}_G, \widehat{f_y}_G).

7. Compute divergence in Fourier space:

   [
   \widehat{\nabla\cdot f}_G
   = i q_x(G),\widehat{f_x}_G + i q_y(G),\widehat{f_y}_G.
   ]

8. Inverse FFT to get A H(r) = −∇·(ε⁻¹∇H).

So TE is “FFT → multiply by |q|² → iFFT”; TM is “FFT → gradient → iFFT → ε⁻¹ → FFT → divergence → iFFT”.

---

#### Checklist — Operator application (TE/TM)

* [ ] Existence & optionality — expose separate code paths for TE/TM matvecs with shared FFT buffers to minimize allocations, and allow profiling-only dry runs.
* [ ] Plotting & logging — log FFT counts per matvec, peak memory usage, and timing breakdown by stage (FFT, ε multiplication, divergence).
* [ ] Data validation — verify A is Hermitian by checking ⟨x,Ay⟩ - ⟨Ax,y⟩ ≈ 0 for random x,y, and ensure B-weighted norms remain positive.
* [ ] Testing implemented — deterministic matvec tests using delta or plane-wave inputs whose outputs are known analytically.

### 2.6 Preconditioner M⁻¹

Goal: approximate (A_k^{-1}) cheaply.

The exact homogeneous-medium operator (ε constant):

* TE: (A_\text{hom} E = -(\nabla+i\mathbf k)^2 E ) ⇒ plane-wave eigenvalues (\lambda_G = |k+G|^2).
* TM: (A_\text{hom} H = -\varepsilon^{-1}_0 (\nabla+i\mathbf k)^2 H) ⇒ (\lambda_G = |k+G|^2 / \varepsilon_0).

So the inverse is **diagonal in Fourier basis**:

* TE:

  [
  (M^{-1} r)_G \approx \frac{1}{|k+G|^2 + \sigma},r_G,
  ]

* TM:

  [
  (M^{-1} r)*G \approx \frac{\varepsilon*\text{eff}}{|k+G|^2 + \sigma},r_G,
  ]

with ε_eff some effective constant (e.g. average of ε or ε⁻¹ over cell), and σ a small shift for stability.

Algorithm (for each residual r):

1. FFT r → r_G.

2. For each G:

   * If |k+G| ≈ 0 (Γ + constant mode): set z_G = 0 (that component is deflated).
   * Else: (z_G = r_G \cdot (\varepsilon_\text{eff} / (|k+G|^2+\sigma))).

3. iFFT z_G → z = M⁻¹ r.

This is what MPB does in spirit: a **Fourier-diagonal preconditioner** using the homogeneous Maxwell operator symbol.

---

#### Checklist — Preconditioner M⁻¹

* [ ] Existence & optionality — allow runtime selection between Fourier-diagonal, Jacobi, or no preconditioner modes and store their configuration in run metadata.
* [ ] Plotting & logging — output spectral statistics (min/max/avg diagonal entries) and histograms of preconditioned residual norms.
* [ ] Data validation — verify the preconditioner commutes with deflation (zeroes constant mode) and never introduces NaNs/Infs due to zero denominators.
* [ ] Testing implemented — convergence comparison tests on toy problems demonstrating speedup over unpreconditioned iterations.

### 2.7 Deflation & constraints

#### Zero/constant mode at Γ

At k=0:

* Constant field u(r)=const → derivative zero → A u = 0 ⇒ λ=0.
* This is a true eigenmode (zero frequency) that you typically don’t want.

Deflation:

* Define constant vector u0, B-normalize it.
* Maintain it in deflation subspace Y.
* Project all vectors (initial guesses, residuals, search directions) onto orthogonal complement:

  [
  v \gets v - u_0 (u_0^* B v).
  ]

#### Multi-band deflation

When computing multiple bands:

* Collect converged eigenvectors (y_1, …, y_p) into Y.
* Ensure they are B-orthonormal: (Y^* B Y = I_p).
* For any new vector v:

  [
  v \gets v - Y (Y^* B v)
  ]

This is the B-orthogonal projector onto the complement of span(Y).

#### Symmetry constraints (optional but powerful)

If the structure has mirror symmetries (e.g. x→−x, y→−y), you can restrict to even/odd subspaces:

* Define symmetry operator S acting on the grid: S u(r) = u(s(r)).
* Even subspace: S u = +u; odd: S u = −u.
* Project:

  [
  u_{\pm} = \frac{1}{2}(u \pm S u).
  ]

Apply this projection after every update to keep iterates in the chosen symmetry subspace.

---

#### Checklist — Deflation & constraints

* [ ] Existence & optionality — enable runtime toggles for constant-mode deflation, previously converged band recycling, and symmetry subspace enforcement.
* [ ] Plotting & logging — log projections’ B-norm errors, deflation subspace ranks, and symmetry classifications applied per k-point.
* [ ] Data validation — ensure deflated vectors remain B-orthonormal via Gram matrices and that symmetry projectors square to themselves numerically.
* [ ] Testing implemented — fixture tests that feed known symmetric/antisymmetric fields and confirm projections behave as expected.

### 2.8 Iterative eigensolver (block LOBPCG, conceptually)

We solve (A_k u = \lambda B u) with A and B Hermitian (A positive definite on the physical subspace).

Block LOBPCG structure for each k:

1. **Initial guess**

   * Take block (X \in \mathbb{C}^{N\times m}) (m = desired bands or slightly more).
   * B-orthonormalize X.
   * Apply deflation and symmetry projections.

2. **Loop until convergence**

For each iteration:

* **Apply operator:**

  [
  W = A_k X.
  ]

* **Rayleigh–Ritz in current subspace:**

  * Compute small matrices:

    * (A_s = X^* W),
    * (B_s = X^* B X) (should be ≈I if X is B-orthonormal),
  * Solve small dense generalized eigenproblem: (A_s C = B_s C \Lambda).
  * Update X ← X C, Λ diagonal of eigenvalues (Rayleigh quotients).

* **Compute residuals:**

  [
  R = W - B X \Lambda.
  ]

* Check norms (|R|_{B}) vs tolerance.

* **Apply deflation & symmetry** to R.

* **Precondition:**

  [
  P = M^{-1} R.
  ]

* B-orthonormalize P against X and Y (deflation subspace).

* **Build new subspace** from columns of X and P, repeat Rayleigh–Ritz.

This is schematic; a concrete LOBPCG has more careful reuse of previous directions, but the logic is:

* A, B, M⁻¹, projectors (deflation + symmetry) are black boxes,
* the eigensolver only needs “apply A”, “apply B”, “apply M⁻¹”, and inner products.

---

#### Checklist — Iterative eigensolver (block LOBPCG)

* [ ] Existence & optionality — offer solver backend selection (LOBPCG, Davidson, ARPACK) and set reasonable defaults per problem size.
* [ ] Plotting & logging — capture iteration traces, relative residuals, orthogonality metrics, and restart events for plotting dashboards.
* [ ] Data validation — verify Ritz vectors remain B-orthonormal and that Rayleigh quotients align with λ estimates each iteration.
* [ ] Testing implemented — end-to-end solver tests on reduced grids comparing eigenvalues/vectors to analytical references.

### 2.9 Band-structure loop

Finally, for the whole band structure:

For each polarization (TE/TM):

* Precompute ε(r), ε⁻¹(r), FFT plans, geometry-dependent constants (ε_eff).
* For each k in k-list:

  1. Build q = k+G, |q|² arrays.
  2. Construct operator A_k and preconditioner M_k.
  3. Set up deflation:

     * constant mode (if k≈0),
     * optionally previous k’s converged eigenvectors as warm start.
  4. Run eigensolver to get lowest N_bands eigenpairs.
  5. Store λ_n(k) → ω_n(k) = c√λ_n.

Outputs are then aggregated into band diagrams.

---

#### Checklist — Band-structure loop

* [ ] Existence & optionality — confirm polarization sweeps can be restricted (TE-only, TM-only, or both) and that k-point batching respects resource limits.
* [ ] Plotting & logging — record per-k timing, convergence status, and live band-gap summaries for dashboards.
* [ ] Data validation — enforce consistent band ordering between neighboring k-points (mode tracking) and verify symmetry-related degeneracies.
* [ ] Testing implemented — integration tests that run the full loop on canonical cells and compare ω_n(k) tables against golden MPB data.

---

That’s the full conceptual pipeline:

* **Master equation** → 2D TE/TM scalar generalized eigenproblems,
* **Geometry** → ε(r) with subpixel smoothing,
* **FFT discretization** → pseudo-spectral A and B operators,
* **Diagonal Fourier preconditioner** from homogeneous limit,
* **Deflation & symmetry projectors** for physical subspace,
* **Block iterative eigensolver** (A,B,M⁻¹, projectors as oracles),
* **Outer loop over k** → band structure.
